{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "KyJltpc_6oEG",
    "outputId": "42223c74-9723-435d-e0a3-bca140cd845b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "jIup-Lmr-l2n",
    "outputId": "ba5043f2-fa2e-4512-bb96-4a0ae17e230d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/My Drive/recom_data/ml-100k\n"
     ]
    }
   ],
   "source": [
    "%cd /content/gdrive/My\\ Drive/recom_data/ml-100k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Y4dtXhpN6mXx"
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim, sigmoid, exp, log, sign\n",
    "from gensim.models.poincare import PoincareModel\n",
    "\n",
    "import pickle\n",
    "from scipy.sparse import csr_matrix, csc_matrix\n",
    "from pathlib import Path\n",
    "import gc\n",
    "\n",
    "from time import time\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML-100k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-22T19:07:10.148206Z",
     "start_time": "2021-05-22T19:07:10.060313Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "id": "mzcH77Mhe0QO",
    "outputId": "93158d1e-868f-4650-d895-33a58c29def8"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/ml-100k/data/u1.base', sep='\\t', header=None)\n",
    "train.columns = 'UserID::MovieID::Rating::Timestamp'.lower().split('::')\n",
    "train.head()\n",
    "\n",
    "val = pd.read_csv('../data/ml-100k/data/u1.test', sep='\\t', header=None)\n",
    "val.columns = 'UserID::MovieID::Rating::Timestamp'.lower().split('::')\n",
    "val.head()\n",
    "\n",
    "ratings_new = train.append(val)\n",
    "ratings_new['userid'], users_keys = ratings_new.userid.factorize()\n",
    "ratings_new['movieid'], movies_keys = ratings_new.movieid.factorize()\n",
    "\n",
    "ratings_new.head()\n",
    "ratings_new = ratings_new.values\n",
    "\n",
    "np.save('users_keys', users_keys)\n",
    "np.save('movies_keys', movies_keys)\n",
    "np.save('ratings_new', ratings_new)\n",
    "\n",
    "train = ratings_new[:len(train)]\n",
    "val = ratings_new[len(train):]\n",
    "\n",
    "np.save('train', train)\n",
    "np.save('val', val)\n",
    "np.save('ratings', ratings_new)\n",
    "\n",
    "shape = [int(ratings_new[:, 0].max()+1), int(ratings_new[:, 1].max()+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kaolvVaffwSP",
    "outputId": "d94d092a-e640-4bff-eafa-b6c923252820",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==1.15.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/98/5a99af92fb911d7a88a0005ad55005f35b4c1ba8d75fba02df726cd936e6/tensorflow-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl (412.3MB)\n",
      "\u001b[K     |████████████████████████████████| 412.3MB 38kB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.1.2)\n",
      "Collecting tensorboard<1.16.0,>=1.15.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8MB 43.8MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.12.1)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (3.12.4)\n",
      "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.36.2)\n",
      "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.8.1)\n",
      "Collecting tensorflow-estimator==1.15.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
      "\u001b[K     |████████████████████████████████| 512kB 56.2MB/s \n",
      "\u001b[?25hCollecting keras-applications>=1.0.8\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 5.7MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.15.0)\n",
      "Collecting gast==0.2.2\n",
      "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
      "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.32.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.19.5)\n",
      "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (3.3.0)\n",
      "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (53.0.0)\n",
      "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.3.3)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.0) (2.10.0)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.4.0)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.4.0)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.7.4.3)\n",
      "Building wheels for collected packages: gast\n",
      "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=2cbe48f4b5a69a633b6c6a82de5e81f5c774062a837162f5720ea3b3856e8816\n",
      "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
      "Successfully built gast\n",
      "\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
      "Installing collected packages: tensorboard, tensorflow-estimator, keras-applications, gast, tensorflow\n",
      "  Found existing installation: tensorboard 2.4.1\n",
      "    Uninstalling tensorboard-2.4.1:\n",
      "      Successfully uninstalled tensorboard-2.4.1\n",
      "  Found existing installation: tensorflow-estimator 2.4.0\n",
      "    Uninstalling tensorflow-estimator-2.4.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
      "  Found existing installation: gast 0.3.3\n",
      "    Uninstalling gast-0.3.3:\n",
      "      Successfully uninstalled gast-0.3.3\n",
      "  Found existing installation: tensorflow 2.4.1\n",
      "    Uninstalling tensorflow-2.4.1:\n",
      "      Successfully uninstalled tensorflow-2.4.1\n",
      "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade tensorflow==1.15.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "okA5oXASaVzm"
   },
   "source": [
    "### This part is copied directly from the authors' repository (with tuned hyperparameters): https://github.com/lorenzMuller/kernelNet_MovieLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "94yyT13nfwdO",
    "outputId": "b37972a1-c357-4acf-c9a8-c256d9f386bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'STOP: TOTAL NO. of ITERATIONS REACHED LIMIT'\n",
      "  Objective function value: 44122.816406\n",
      "  Number of iterations: 50\n",
      "  Number of functions evaluations: 54\n",
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'STOP: TOTAL NO. of ITERATIONS REACHED LIMIT'\n",
      "  Objective function value: 39787.929688\n",
      "  Number of iterations: 50\n",
      "  Number of functions evaluations: 54\n",
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'STOP: TOTAL NO. of ITERATIONS REACHED LIMIT'\n",
      "  Objective function value: 38976.421875\n",
      "  Number of iterations: 50\n",
      "  Number of functions evaluations: 54\n",
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'STOP: TOTAL NO. of ITERATIONS REACHED LIMIT'\n",
      "  Objective function value: 38602.019531\n",
      "  Number of iterations: 50\n",
      "  Number of functions evaluations: 53\n",
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'STOP: TOTAL NO. of ITERATIONS REACHED LIMIT'\n",
      "  Objective function value: 38328.628906\n",
      "  Number of iterations: 50\n",
      "  Number of functions evaluations: 55\n",
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'STOP: TOTAL NO. of ITERATIONS REACHED LIMIT'\n",
      "  Objective function value: 37736.367188\n",
      "  Number of iterations: 50\n",
      "  Number of functions evaluations: 53\n",
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'STOP: TOTAL NO. of ITERATIONS REACHED LIMIT'\n",
      "  Objective function value: 37260.742188\n",
      "  Number of iterations: 50\n",
      "  Number of functions evaluations: 53\n",
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'STOP: TOTAL NO. of ITERATIONS REACHED LIMIT'\n",
      "  Objective function value: 36925.039062\n",
      "  Number of iterations: 50\n",
      "  Number of functions evaluations: 54\n",
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'STOP: TOTAL NO. of ITERATIONS REACHED LIMIT'\n",
      "  Objective function value: 36512.046875\n",
      "  Number of iterations: 50\n",
      "  Number of functions evaluations: 54\n",
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'STOP: TOTAL NO. of ITERATIONS REACHED LIMIT'\n",
      "  Objective function value: 36274.437500\n",
      "  Number of iterations: 50\n",
      "  Number of functions evaluations: 55\n",
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'STOP: TOTAL NO. of ITERATIONS REACHED LIMIT'\n",
      "  Objective function value: 36119.902344\n",
      "  Number of iterations: 50\n",
      "  Number of functions evaluations: 53\n",
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'STOP: TOTAL NO. of ITERATIONS REACHED LIMIT'\n",
      "  Objective function value: 35977.562500\n",
      "  Number of iterations: 50\n",
      "  Number of functions evaluations: 53\n",
      "Fit time: 62.54027438163757 seconds\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._ \n",
      " epoch: 11 validation rmse: 0.8994563467669021 train rmse: 0.8121772747098162 \n",
      " .-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "672773\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "# load data\n",
    "tr = csr_matrix((train[:, 2], (train[:, 0], train[:, 1])), shape=shape).toarray().T\n",
    "vr = csr_matrix((val[:, 2], (val[:, 0], val[:, 1])), shape=shape).toarray().T\n",
    "\n",
    "tm = np.greater(tr, 1e-12).astype('float32')  # masks indicating non-zero entries\n",
    "vm = np.greater(vr, 1e-12).astype('float32')\n",
    "\n",
    "n_m = tr.shape[0]  # number of movies\n",
    "n_u = tr.shape[1]  # number of users (may be switched depending on 'transpose' in loadData)\n",
    "\n",
    "# Set hyper-parameters\n",
    "n_hid = 300\n",
    "lambda_2 = 40.\n",
    "lambda_s = 0.02\n",
    "n_layers = 2\n",
    "output_every = 50  # evaluate performance on test set; breaks l-bfgs loop\n",
    "n_epoch = n_layers * 10 * output_every\n",
    "verbose_bfgs = False\n",
    "use_gpu = True\n",
    "if not use_gpu:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "    \n",
    "# Input placeholders\n",
    "R = tf.placeholder(\"float\", [None, n_u])\n",
    "\n",
    "# define network functions\n",
    "def kernel(u, v):\n",
    "    \"\"\"\n",
    "    Sparsifying kernel function\n",
    "    :param u: input vectors [n_in, 1, n_dim]\n",
    "    :param v: output vectors [1, n_hid, n_dim]\n",
    "    :return: input to output connection matrix\n",
    "    \"\"\"\n",
    "    dist = tf.norm(u - v, ord=2, axis=2)\n",
    "    hat = tf.maximum(0., 1. - dist**2)\n",
    "    return hat\n",
    "\n",
    "\n",
    "def kernel_layer(x, n_hid=500, n_dim=5, activation=tf.nn.sigmoid, lambda_s=lambda_s,\n",
    "                lambda_2=lambda_2, name=''):\n",
    "    \"\"\"\n",
    "    a kernel sparsified layer\n",
    "    :param x: input [batch, channels]\n",
    "    :param n_hid: number of hidden units\n",
    "    :param n_dim: number of dimensions to embed for kernelization\n",
    "    :param activation: output activation\n",
    "    :param name: layer name for scoping\n",
    "    :return: layer output, regularization term\n",
    "    \"\"\"\n",
    "\n",
    "    # define variables\n",
    "    with tf.variable_scope(name):\n",
    "        W = tf.get_variable('W', [x.shape[1], n_hid])\n",
    "        n_in = x.get_shape().as_list()[1]\n",
    "        u = tf.get_variable('u', initializer=tf.random.truncated_normal([n_in, 1, n_dim], 0., 1e-3))\n",
    "        v = tf.get_variable('v', initializer=tf.random.truncated_normal([1, n_hid, n_dim], 0., 1e-3))\n",
    "        b = tf.get_variable('b', [n_hid])\n",
    "\n",
    "    # compute sparsifying kernel\n",
    "    # as u and v move further from each other for some given pair of neurons, their connection\n",
    "    # decreases in strength and eventually goes to zero.\n",
    "    w_hat = kernel(u, v)\n",
    "\n",
    "    # compute regularization terms\n",
    "    sparse_reg = tf.contrib.layers.l2_regularizer(lambda_s)\n",
    "    sparse_reg_term = tf.contrib.layers.apply_regularization(sparse_reg, [w_hat])\n",
    "\n",
    "    l2_reg = tf.contrib.layers.l2_regularizer(lambda_2)\n",
    "    l2_reg_term = tf.contrib.layers.apply_regularization(l2_reg, [W])\n",
    "\n",
    "    # compute output\n",
    "    W_eff = W * w_hat\n",
    "    y = tf.matmul(x, W_eff) + b\n",
    "    y = activation(y)\n",
    "    return y, sparse_reg_term + l2_reg_term\n",
    "\n",
    "\n",
    "# Instantiate network\n",
    "y = R\n",
    "reg_losses = None\n",
    "for i in range(n_layers):\n",
    "    y, reg_loss = kernel_layer(y, n_hid, name=str(i))\n",
    "    reg_losses = reg_loss if reg_losses is None else reg_losses + reg_loss\n",
    "prediction, reg_loss = kernel_layer(y, n_u, activation=tf.identity, name='out')\n",
    "reg_losses = reg_losses + reg_loss\n",
    "\n",
    "# Compute loss (symbolic)\n",
    "diff = tm*(R - prediction)\n",
    "sqE = tf.nn.l2_loss(diff)\n",
    "\n",
    "loss = sqE + reg_losses\n",
    "\n",
    "# Instantiate L-BFGS Optimizer\n",
    "optimizer = tf.contrib.opt.ScipyOptimizerInterface(loss, options={'maxiter': output_every,\n",
    "                                                                  'disp': verbose_bfgs,\n",
    "                                                                  'maxcor': 10},\n",
    "                                                  method='L-BFGS-B')\n",
    "\n",
    "# Training and validation loop\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "time_start = time()\n",
    "for i in range(12):\n",
    "    optimizer.minimize(sess, feed_dict={R: tr}) #do maxiter optimization steps\n",
    "\n",
    "time_fit = time() - time_start\n",
    "print(f'Fit time: {time_fit} seconds')\n",
    "\n",
    "pre = sess.run(prediction, feed_dict={R: tr}) #predict ratings\n",
    "error = (vm * (np.clip(pre, 1., 5.) - vr) ** 2).sum() / vm.sum() #compute validation error\n",
    "error_train = (tm * (np.clip(pre, 1., 5.) - tr) ** 2).sum() / tm.sum() #compute train error\n",
    "print('.-^-._' * 12, '\\n', 'epoch:', i, 'validation rmse:', np.sqrt(error), 'train rmse:', np.sqrt(error_train), '\\n', '.-^-._' * 12)\n",
    "\n",
    "total_parameters = 0\n",
    "for variable in tf.trainable_variables():\n",
    "    # shape is an array of tf.Dimension\n",
    "    shape = variable.get_shape()\n",
    "    variable_parameters = 1\n",
    "    for dim in shape:\n",
    "        variable_parameters *= dim.value\n",
    "    total_parameters += variable_parameters\n",
    "print(total_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pRU2zos_o6Ni"
   },
   "source": [
    "## Douban"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lUVSeoEmxQUT",
    "outputId": "54bed9be-3088-4308-b73c-5b395db6c387"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/My Drive/recom_data/douban\n"
     ]
    }
   ],
   "source": [
    "%cd /content/gdrive/My Drive/recom_data/douban"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "FOP5sGFnxQYf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ratings = np.load('../data/douban/data/douban.npy', allow_pickle=True)\n",
    "train = np.load('../data/douban/data/otraining.npy', allow_pickle=True) * ratings\n",
    "val = np.load('../data/douban/data/otest.npy', allow_pickle=True) * ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U46Wx1T6bRiD"
   },
   "source": [
    "### This part is copied directly from the authors' repository (with tuned hyperparameters): https://github.com/lorenzMuller/kernelNet_MovieLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C_fz1DqWp8MF",
    "outputId": "c39f39f7-d8e1-4aec-f953-f52a872dfcb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'STOP: TOTAL NO. of ITERATIONS REACHED LIMIT'\n",
      "  Objective function value: 46422.539062\n",
      "  Number of iterations: 50\n",
      "  Number of functions evaluations: 53\n",
      "Fit time: 15.723491191864014 seconds\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._ \n",
      " epoch: 0 validation rmse: 0.73098534 train rmse: 0.57484835 \n",
      " .-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "1836300\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "tf.reset_default_graph()\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "# load data\n",
    "tr = train.copy().T\n",
    "vr = val.copy().T\n",
    "\n",
    "tm = np.greater(tr, 1e-12).astype('float32')  # masks indicating non-zero entries\n",
    "vm = np.greater(vr, 1e-12).astype('float32')\n",
    "\n",
    "n_m = tr.shape[0]  # number of movies\n",
    "n_u = tr.shape[1]  # number of users (may be switched depending on 'transpose' in loadData)\n",
    "\n",
    "# Set hyper-parameters\n",
    "n_hid = 300\n",
    "lambda_2 = 40.\n",
    "lambda_s = 0.013\n",
    "n_layers = 1\n",
    "output_every = 50  # evaluate performance on test set; breaks l-bfgs loop\n",
    "n_epoch = n_layers * 10 * output_every\n",
    "verbose_bfgs = False\n",
    "use_gpu = True\n",
    "if not use_gpu:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "    \n",
    "# Input placeholders\n",
    "R = tf.placeholder(\"float\", [None, n_u])\n",
    "\n",
    "# define network functions\n",
    "def kernel(u, v):\n",
    "    \"\"\"\n",
    "    Sparsifying kernel function\n",
    "    :param u: input vectors [n_in, 1, n_dim]\n",
    "    :param v: output vectors [1, n_hid, n_dim]\n",
    "    :return: input to output connection matrix\n",
    "    \"\"\"\n",
    "    dist = tf.norm(u - v, ord=2, axis=2)\n",
    "    hat = tf.maximum(0., 1. - dist**2)\n",
    "    return hat\n",
    "\n",
    "\n",
    "def kernel_layer(x, n_hid=500, n_dim=5, activation=tf.nn.sigmoid, lambda_s=lambda_s,\n",
    "                lambda_2=lambda_2, name=''):\n",
    "    \"\"\"\n",
    "    a kernel sparsified layer\n",
    "    :param x: input [batch, channels]\n",
    "    :param n_hid: number of hidden units\n",
    "    :param n_dim: number of dimensions to embed for kernelization\n",
    "    :param activation: output activation\n",
    "    :param name: layer name for scoping\n",
    "    :return: layer output, regularization term\n",
    "    \"\"\"\n",
    "\n",
    "    # define variables\n",
    "    with tf.variable_scope(name):\n",
    "        W = tf.get_variable('W', [x.shape[1], n_hid])\n",
    "        n_in = x.get_shape().as_list()[1]\n",
    "        u = tf.get_variable('u', initializer=tf.random.truncated_normal([n_in, 1, n_dim], 0., 1e-3))\n",
    "        v = tf.get_variable('v', initializer=tf.random.truncated_normal([1, n_hid, n_dim], 0., 1e-3))\n",
    "        b = tf.get_variable('b', [n_hid])\n",
    "\n",
    "    # compute sparsifying kernel\n",
    "    # as u and v move further from each other for some given pair of neurons, their connection\n",
    "    # decreases in strength and eventually goes to zero.\n",
    "    w_hat = kernel(u, v)\n",
    "\n",
    "    # compute regularization terms\n",
    "    sparse_reg = tf.contrib.layers.l2_regularizer(lambda_s)\n",
    "    sparse_reg_term = tf.contrib.layers.apply_regularization(sparse_reg, [w_hat])\n",
    "\n",
    "    l2_reg = tf.contrib.layers.l2_regularizer(lambda_2)\n",
    "    l2_reg_term = tf.contrib.layers.apply_regularization(l2_reg, [W])\n",
    "\n",
    "    # compute output\n",
    "    W_eff = W * w_hat\n",
    "    y = tf.matmul(x, W_eff) + b\n",
    "    y = activation(y)\n",
    "    return y, sparse_reg_term + l2_reg_term\n",
    "\n",
    "\n",
    "# Instantiate network\n",
    "y = R\n",
    "reg_losses = None\n",
    "for i in range(n_layers):\n",
    "    y, reg_loss = kernel_layer(y, n_hid, name=str(i))\n",
    "    reg_losses = reg_loss if reg_losses is None else reg_losses + reg_loss\n",
    "prediction, reg_loss = kernel_layer(y, n_u, activation=tf.identity, name='out')\n",
    "reg_losses = reg_losses + reg_loss\n",
    "\n",
    "# Compute loss (symbolic)\n",
    "diff = tm*(R - prediction)\n",
    "sqE = tf.nn.l2_loss(diff)\n",
    "\n",
    "loss = sqE + reg_losses\n",
    "\n",
    "# Instantiate L-BFGS Optimizer\n",
    "optimizer = tf.contrib.opt.ScipyOptimizerInterface(loss, options={'maxiter': output_every,\n",
    "                                                                  'disp': verbose_bfgs,\n",
    "                                                                  'maxcor': 5},\n",
    "                                                  method='L-BFGS-B')\n",
    "\n",
    "# Training and validation loop\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "time_start = time()\n",
    "for i in range(1):\n",
    "    optimizer.minimize(sess, feed_dict={R: tr}) #do maxiter optimization steps\n",
    "\n",
    "time_fit = time() - time_start\n",
    "print(f'Fit time: {time_fit} seconds')\n",
    "\n",
    "pre = sess.run(prediction, feed_dict={R: tr}) #predict ratings\n",
    "error = (vm * (np.clip(pre, 1., 5.) - vr) ** 2).sum() / vm.sum() #compute validation error\n",
    "error_train = (tm * (np.clip(pre, 1., 5.) - tr) ** 2).sum() / tm.sum() #compute train error\n",
    "print('.-^-._' * 12, '\\n', 'epoch:', i, 'validation rmse:', np.sqrt(error), 'train rmse:', np.sqrt(error_train), '\\n', '.-^-._' * 12)\n",
    "\n",
    "total_parameters = 0\n",
    "for variable in tf.trainable_variables():\n",
    "    # shape is an array of tf.Dimension\n",
    "    shape = variable.get_shape()\n",
    "    variable_parameters = 1\n",
    "    for dim in shape:\n",
    "        variable_parameters *= dim.value\n",
    "    total_parameters += variable_parameters\n",
    "print(total_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HfbA-sc5c7Zt"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "sparse_fc.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
